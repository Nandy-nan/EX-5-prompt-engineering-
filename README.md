


# EXP 5: Comparative Analysis of Na√Øve Prompting versus Basic Prompting Using ChatGPT Across Various Test Scenarios
# Aim: To test how ChatGPT responds to na√Øve prompts (broad or unstructured) versus basic prompts (clearer and more refined) across multiple scenarios, analyzing the quality, accuracy, and depth of the generated responses.
# Algorithm: 
üß† Algorithm: Comparative Prompting Analysis
Step 1: Define Prompt Types
Na√Øve Prompts:
Broad, vague, or open-ended. Lack structure or context.
üîπ Example: ‚ÄúTell me a story.‚Äù

Basic Prompts:
Clear, detailed, and specific. Provide context and direction.
üîπ Example: ‚ÄúWrite a 300-word bedtime story for a 6-year-old about a turtle who finds a magical island.‚Äù

Step 2: Select Test Scenarios
Choose a range of use cases that test different skills of the model. Suggested scenarios:

Creative Story Generation

Factual Question Answering

Article or Concept Summarization

Advice or Recommendation

Code Generation

Explanation of a Complex Topic

List/Planning Task

Visual Description (for models with multimodal capabilities)

Step 3: Create Prompt Pairs
Prepare two prompts for each test scenario:

One na√Øve prompt: broad, vague, or open-ended.

One basic prompt: clear, structured, and specific.

Ensure each pair targets the same task but with different prompt clarity levels.

Example for a story generation task:

Na√Øve Prompt: ‚ÄúTell me a story.‚Äù

Basic Prompt: ‚ÄúWrite a bedtime story about a shy penguin who learns to swim.‚Äù

Step 4: Run Prompts through ChatGPT
Input both prompts into ChatGPT for each scenario.

Collect and store both outputs side by side.

Repeat across all selected test cases to build a consistent dataset.

Step 5: Evaluate & Analyze Responses
Compare responses based on these key metrics:

Quality: Is the response well-written and coherent?

Accuracy: Are the facts or logic correct (where applicable)?

Depth: Does the response provide detail, insight, or creativity?

Assess whether:

Basic prompts consistently lead to better results.

Any scenarios show na√Øve prompts performing equally well.

Improvements in prompt design lead to more user-satisfying outputs.

Sample Comparative Results (As Points)
Scenario: Story Generation
Na√Øve Prompt:

Quality: Moderate

Accuracy: High

Depth: Shallow

Notes: Short, general story without a strong theme

Basic Prompt:

Quality: Excellent

Accuracy: High

Depth: Deep

Notes: Creative story with clear plot and message

Scenario: Factual Question
Na√Øve Prompt:

Quality: Good

Accuracy: Moderate

Depth: Low

Notes: Some key facts missing

Basic Prompt:

Quality: Excellent

Accuracy: High

Depth: Moderate

Notes: Covers important points with explanations

Scenario: Concept Summary
Na√Øve Prompt:

Quality: Good

Accuracy: High

Depth: Medium

Notes: Lacks simplification for younger audience

Basic Prompt:

Quality: Excellent

Accuracy: High

Depth: High

Notes: Well simplified for 10-year-olds

Scenario: Code Generation
Na√Øve Prompt:

Quality: Acceptable

Accuracy: Moderate

Depth: Low

Notes: Generic snippet without comments

Basic Prompt:

Quality: Excellent

Accuracy: High

Depth: High

Notes: Clean code with comments and examples

üß© Insights and Analysis
‚úÖ Findings:
Basic prompts consistently led to better responses in terms of:

Relevance

Clarity

User satisfaction

Na√Øve prompts worked acceptably in:

Open-ended creativity (storytelling, brainstorming)

Casual or exploratory tasks (e.g., ‚Äúsurprise me‚Äù situations)

‚ùó Challenges with Na√Øve Prompts:
May lead to generic or vague responses

Require follow-up prompts to narrow down focus

Depend more on model inference and assumptions

üí° Prompt Design Tip:
"The more precise your prompt, the more useful the response."
Adding who, what, when, how, and why elements can significantly improve ChatGPT's output.

üìÅ Deliverables
‚úÖ Comparative Table of Prompt Types Across All Scenarios

‚úÖ Evaluation Report (Quality, Accuracy, Depth metrics)

‚úÖ Summary of Findings on Effective Prompting Patterns

(Optional) JSON/CSV log of responses for reproducibility

(Optional) GitHub repo structure or documentation template



# OUTPUT
Scenario: Story Generation
Na√Øve Prompt:
‚ÄúTell me a story.‚Äù

ChatGPT Output (Na√Øve):
Once upon a time, there was a happy little bird who lived in a forest. The bird sang every day and made many friends. One day, it flew to a new place and discovered a beautiful lake. The end.

Basic Prompt:
‚ÄúWrite a bedtime story about a shy penguin who learns to swim and makes new friends in the icy ocean.‚Äù

ChatGPT Output (Basic):
In the chilly waters of the Antarctic, there lived a shy little penguin named Pip. Pip was afraid of swimming because the ocean seemed so big and scary. Every day, he watched his friends glide through the icy waves with joy. One chilly morning, Pip decided to be brave. Slowly, he dipped his flippers into the water and paddled a little. With practice and encouragement from his friends, Pip learned to swim like a pro. Soon, he was playing hide-and-seek under the ice and making new friends among the playful seals. That night, as Pip snuggled under the stars, he knew that sometimes being brave brings the best adventures.



# RESULT: The prompt for the above said problem executed successfully





